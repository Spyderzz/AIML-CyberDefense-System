{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Setup & imports\n",
    "import os, json, glob, math, random, joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "\n",
    "ISIT_ROOT = os.path.join(ROOT, \"data\", \"raw\", \"ISIT_Dataset\")\n",
    "BALABIT_ROOT = os.path.join(ROOT, \"data\", \"raw\", \"Balabit_dataset_refined\")\n",
    "OUT_DIR = os.path.join(ROOT, \"data\", \"processed\")\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# CPU / TF thread tuning (safe for notebook)\n",
    "import tensorflow as tf\n",
    "seed = 42\n",
    "np.random.seed(seed); random.seed(seed); tf.random.set_seed(seed)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "os.environ['MKL_NUM_THREADS'] = '6'\n",
    "tf.config.threading.set_intra_op_parallelism_threads(6)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "print(\"Paths set. OUT_DIR:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement loader cell for ISIT + Balabit (handles ISIT.trace format and Balabit CSVs w/o timestamps)\n",
    "import os, glob, json, pandas as pd, math\n",
    "from typing import List, Tuple, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# where your roots were set previously\n",
    "# ISIT_ROOT, BALABIT_ROOT must already be defined in Cell 1\n",
    "SEARCH_EXTS = (\".json\", \".jsonl\", \".ndjson\", \".csv\", \".txt\", \".log\")\n",
    "\n",
    "def find_files_recursive(root, exts=SEARCH_EXTS):\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += glob.glob(os.path.join(root, \"**\", f\"*{ext}\"), recursive=True)\n",
    "    files = sorted(list(dict.fromkeys(files)))\n",
    "    return files\n",
    "\n",
    "def parse_isit_json_obj(obj):\n",
    "    \"\"\"\n",
    "    Parse ISIT JSON structure example seen in your logs:\n",
    "    {\n",
    "      \"user_id\": \"...\",\n",
    "      \"device\": \"desktop\",\n",
    "      \"trace\": [\n",
    "        {\"event_name\":\"load\",\"timestamp\":\"1695...\",\"position\":{\"x\":0,\"y\":0}},\n",
    "        {\"event_name\":\"mousemove\",\"timestamp\":\"1695...\",\"position\":{\"x\":123,\"y\":456}},\n",
    "        ...\n",
    "      ],\n",
    "      ...maybe other keys...\n",
    "    }\n",
    "    Returns a list of {\"x\", \"y\", \"t\"} events or None if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "    trace = None\n",
    "    for k in (\"trace\", \"events\", \"mouse\", \"traj\"):\n",
    "        if k in obj:\n",
    "            trace = obj[k]; break\n",
    "    if trace is None or not isinstance(trace, list):\n",
    "        return None\n",
    "    out = []\n",
    "    for e in trace:\n",
    "        # timestamp might be string numeric ms or nested; position might be nested\n",
    "        t = None\n",
    "        if isinstance(e, dict):\n",
    "            # some traces use 'timestamp' or 'time' or 'ts'\n",
    "            for tk in (\"timestamp\",\"time\",\"ts\",\"t\"):\n",
    "                if tk in e:\n",
    "                    t = e.get(tk); break\n",
    "            # position might be in e['position'] or e.get('pos')\n",
    "            pos = None\n",
    "            if \"position\" in e and isinstance(e[\"position\"], dict):\n",
    "                pos = e[\"position\"]\n",
    "            elif \"pos\" in e and isinstance(e[\"pos\"], dict):\n",
    "                pos = e[\"pos\"]\n",
    "            else:\n",
    "                # sometimes coordinates are top-level keys 'x','y'\n",
    "                pos = e\n",
    "            # read x,y if available\n",
    "            try:\n",
    "                x = pos.get(\"x\", pos.get(\"X\")) if isinstance(pos, dict) else None\n",
    "                y = pos.get(\"y\", pos.get(\"Y\")) if isinstance(pos, dict) else None\n",
    "            except Exception:\n",
    "                x = None; y = None\n",
    "            # fallback: e may be like {'clientX':..}\n",
    "            if x is None or y is None:\n",
    "                for cand in (\"clientX\",\"clientY\",\"pageX\",\"pageY\"):\n",
    "                    if cand in e:\n",
    "                        if cand.endswith(\"X\"): x = e[cand]\n",
    "                        if cand.endswith(\"Y\"): y = e[cand]\n",
    "        else:\n",
    "            continue\n",
    "        # convert timestamp to float ms if string\n",
    "        try:\n",
    "            if t is None:\n",
    "                # some traces have 'timeOrigin' at top-level or events may not have timestamps\n",
    "                t = e.get(\"time\") if isinstance(e, dict) else None\n",
    "            if isinstance(t, str) and t.isdigit():\n",
    "                t = float(t)\n",
    "            else:\n",
    "                try:\n",
    "                    t = float(t)\n",
    "                except:\n",
    "                    t = None\n",
    "        except Exception:\n",
    "            t = None\n",
    "        # if coordinates present and numeric, append\n",
    "        try:\n",
    "            x_f = float(x)\n",
    "            y_f = float(y)\n",
    "            if t is None:\n",
    "                # if no timestamp, we'll append None and caller can synthesize later\n",
    "                out.append({\"x\": x_f, \"y\": y_f, \"t\": None})\n",
    "            else:\n",
    "                out.append({\"x\": x_f, \"y\": y_f, \"t\": float(t)})\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out if out else None\n",
    "\n",
    "def parse_balabit_csv(path):\n",
    "    \"\"\"\n",
    "    Balabit CSVs appear to contain only x,y per row (no timestamp).\n",
    "    We will read x,y and create a synthetic timestamp t = index * delta_ms.\n",
    "    delta_ms default = 16ms (~60Hz); you can adjust if you know sampling rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=0)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # identify x,y columns (names may vary)\n",
    "    xcol = None; ycol = None\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if cl in (\"x\",\"posx\",\"clientx\",\"pagex\"): xcol = c\n",
    "        if cl in (\"y\",\"posy\",\"clienty\",\"pagey\"): ycol = c\n",
    "    # if only two unnamed columns, treat them as x,y\n",
    "    if xcol is None or ycol is None:\n",
    "        # try first two numeric columns\n",
    "        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if len(numeric_cols) >= 2:\n",
    "            xcol, ycol = numeric_cols[0], numeric_cols[1]\n",
    "        else:\n",
    "            # fallback: if there are exactly 2 columns, use them\n",
    "            if len(df.columns) == 2:\n",
    "                xcol, ycol = df.columns[0], df.columns[1]\n",
    "            else:\n",
    "                return None\n",
    "    xs = df[xcol].astype(float).tolist()\n",
    "    ys = df[ycol].astype(float).tolist()\n",
    "    n = len(xs)\n",
    "    # create synthetic timestamps: delta_ms can be tuned. 16 ms ~ 60Hz; 10ms faster sampling.\n",
    "    delta_ms = 16.0\n",
    "    # If there's an index column that looks like timestamp, detect it:\n",
    "    possible_time = None\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"time\" in cl or \"timestamp\" in cl or \"ts\" == cl:\n",
    "            possible_time = c; break\n",
    "    if possible_time:\n",
    "        ts = df[possible_time].astype(float).tolist()\n",
    "        events = [{\"x\": float(xs[i]), \"y\": float(ys[i]), \"t\": float(ts[i]) if not (pd.isna(ts[i])) else i*delta_ms} for i in range(n)]\n",
    "    else:\n",
    "        events = [{\"x\": float(xs[i]), \"y\": float(ys[i]), \"t\": i*delta_ms} for i in range(n)]\n",
    "    return events\n",
    "\n",
    "def load_sessions_from_root(root):\n",
    "    files = find_files_recursive(root)\n",
    "    print(f\"Scanning {len(files)} files under {root}\")\n",
    "    parsed = []\n",
    "    for f in files:\n",
    "        ext = Path(f).suffix.lower()\n",
    "        if ext in (\".json\", \".jsonl\", \".ndjson\"):\n",
    "            try:\n",
    "                with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                    txt = fh.read()\n",
    "                # try parse JSON or JSON lines\n",
    "                try:\n",
    "                    obj = json.loads(txt)\n",
    "                    # If obj is a dict matching ISIT structure -> parse trace\n",
    "                    if isinstance(obj, dict):\n",
    "                        evs = parse_isit_json_obj(obj)\n",
    "                        if evs:\n",
    "                            parsed.append((evs, None, f))\n",
    "                            continue\n",
    "                        # else maybe it's a list of sessions\n",
    "                        # fall through to handle below\n",
    "                    if isinstance(obj, list):\n",
    "                        # each item may be a session or event list\n",
    "                        for item in obj:\n",
    "                            if isinstance(item, dict):\n",
    "                                evs = parse_isit_json_obj(item) or (item.get(\"events\") or item.get(\"mouse\") or item.get(\"trace\"))\n",
    "                                if evs:\n",
    "                                    parsed.append((evs, None, f))\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    # try jsonlines\n",
    "                    items = []\n",
    "                    for line in txt.splitlines():\n",
    "                        line = line.strip()\n",
    "                        if not line: continue\n",
    "                        try:\n",
    "                            items.append(json.loads(line))\n",
    "                        except:\n",
    "                            continue\n",
    "                    for item in items:\n",
    "                        evs = parse_isit_json_obj(item) or (item.get(\"events\") if isinstance(item, dict) else None)\n",
    "                        if evs:\n",
    "                            parsed.append((evs, None, f))\n",
    "                            continue\n",
    "            except Exception as e:\n",
    "                # skip file\n",
    "                continue\n",
    "        elif ext == \".csv\":\n",
    "            evs = parse_balabit_csv(f)\n",
    "            if evs:\n",
    "                parsed.append((evs, None, f))\n",
    "                continue\n",
    "        else:\n",
    "            # try to open and see if it contains JSON lines\n",
    "            try:\n",
    "                with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                    txt = fh.read()\n",
    "                items = []\n",
    "                try:\n",
    "                    items = json.loads(txt)\n",
    "                except:\n",
    "                    for line in txt.splitlines():\n",
    "                        line = line.strip()\n",
    "                        if not line: continue\n",
    "                        try:\n",
    "                            items.append(json.loads(line))\n",
    "                        except:\n",
    "                            continue\n",
    "                for item in items:\n",
    "                    evs = parse_isit_json_obj(item) or (item.get(\"events\") if isinstance(item, dict) else None)\n",
    "                    if evs:\n",
    "                        parsed.append((evs, None, f))\n",
    "            except:\n",
    "                continue\n",
    "    print(f\"Raw parsed session-like items: {len(parsed)}\")\n",
    "    # show some examples and infer labels from path heuristics\n",
    "    samples = parsed[:8]\n",
    "    for i, (evs, lbl, p) in enumerate(samples):\n",
    "        print(f\"[{i}] path={p} events={len(evs)} sample_event={evs[0] if evs else None}\")\n",
    "    # infer labels from path heuristics and return normalized sessions\n",
    "    def infer_label_from_path(p):\n",
    "        pl = p.lower()\n",
    "        if any(k in pl for k in (\"gremlin\",\"gremlins\",\"za_proxy\",\"random_mouse\",\"sleep_bot\",\"fake\",\"fake_data\",\"bot\",\"proxy\")):\n",
    "            return \"bot\"\n",
    "        if any(k in pl for k in (\"hlisa\",\"survey\",\"desktop\",\"training_files\",\"test_files\",\"balabit\",\"real\")):\n",
    "            return \"human\"\n",
    "        # default conservative = human\n",
    "        return \"human\"\n",
    "    out_sessions = []\n",
    "    for evs, lbl, p in parsed:\n",
    "        # evs is list of dicts possibly with 't' None for some ISIT entries; we will synthesize if needed\n",
    "        # ensure events are dicts with numeric x,y,t; convert timestamp strings to float; if t missing, set to None (synth later)\n",
    "        normalized = []\n",
    "        for e in evs:\n",
    "            try:\n",
    "                if isinstance(e, dict):\n",
    "                    x = e.get(\"x\") if \"x\" in e else (e.get(\"position\",{}).get(\"x\") if isinstance(e.get(\"position\"), dict) else None)\n",
    "                    y = e.get(\"y\") if \"y\" in e else (e.get(\"position\",{}).get(\"y\") if isinstance(e.get(\"position\"), dict) else None)\n",
    "                    t = e.get(\"t\") if \"t\" in e else e.get(\"timestamp\", e.get(\"time\", None))\n",
    "                    # sometimes t is string digits: convert\n",
    "                    if isinstance(t, str) and t.isdigit():\n",
    "                        t = float(t)\n",
    "                    try:\n",
    "                        x_f = float(x)\n",
    "                        y_f = float(y)\n",
    "                    except:\n",
    "                        # skip if coords missing\n",
    "                        continue\n",
    "                    if t is None:\n",
    "                        normalized.append({\"x\": x_f, \"y\": y_f, \"t\": None})\n",
    "                    else:\n",
    "                        try:\n",
    "                            t_f = float(t)\n",
    "                        except:\n",
    "                            t_f = None\n",
    "                        normalized.append({\"x\": x_f, \"y\": y_f, \"t\": t_f})\n",
    "                elif isinstance(e, (list, tuple)) and len(e) >= 2:\n",
    "                    x_f = float(e[0]); y_f = float(e[1])\n",
    "                    t_f = float(e[2]) if len(e) >= 3 else None\n",
    "                    normalized.append({\"x\": x_f, \"y\": y_f, \"t\": t_f})\n",
    "            except Exception:\n",
    "                continue\n",
    "        if len(normalized) < 3:\n",
    "            continue\n",
    "        label = lbl if lbl is not None else infer_label_from_path(p)\n",
    "        out_sessions.append((normalized, label, p))\n",
    "    print(f\"Normalized usable sessions: {len(out_sessions)}\")\n",
    "    return out_sessions\n",
    "\n",
    "# Run for both roots\n",
    "print(\"Parsing ISIT root...\")\n",
    "isit_sessions = load_sessions_from_root(ISIT_ROOT)\n",
    "print(\"Parsing Balabit root...\")\n",
    "balabit_sessions = load_sessions_from_root(BALABIT_ROOT)\n",
    "\n",
    "print(f\"ISIT usable sessions: {len(isit_sessions)}\")\n",
    "print(f\"Balabit usable sessions: {len(balabit_sessions)}\")\n",
    "\n",
    "# Combine\n",
    "all_sessions = isit_sessions + balabit_sessions\n",
    "print(\"TOTAL usable sessions:\", len(all_sessions))\n",
    "\n",
    "# At this point some events may have 't': None. We will synthesize missing timestamps (per-session)\n",
    "def synthesize_timestamps_if_needed(events, default_delta_ms=16.0):\n",
    "    # If any event has t == None, create monotonic timestamps using provided deltas\n",
    "    if any(e.get(\"t\") is None for e in events):\n",
    "        tvals = []\n",
    "        # If some events have t and some not, try to anchor; otherwise use index*delta\n",
    "        existing = [e.get(\"t\") for e in events if e.get(\"t\") is not None]\n",
    "        if existing:\n",
    "            # anchor first known as offset; fill gaps using delta\n",
    "            # find first index where t present\n",
    "            first_idx = next(i for i,e in enumerate(events) if e.get(\"t\") is not None)\n",
    "            # build timestamps relative to that\n",
    "            for i in range(len(events)):\n",
    "                if i <= first_idx:\n",
    "                    tvals.append(events[first_idx][\"t\"] - (first_idx - i) * default_delta_ms)\n",
    "                else:\n",
    "                    prev = tvals[-1]\n",
    "                    tvals.append(prev + default_delta_ms)\n",
    "        else:\n",
    "            tvals = [i * default_delta_ms for i in range(len(events))]\n",
    "        # assign\n",
    "        for i, e in enumerate(events):\n",
    "            e[\"t\"] = float(tvals[i])\n",
    "    # else: ensure t are floats\n",
    "    for e in events:\n",
    "        if not isinstance(e.get(\"t\"), (int,float)):\n",
    "            try:\n",
    "                e[\"t\"] = float(e[\"t\"])\n",
    "            except:\n",
    "                e[\"t\"] = 0.0\n",
    "    return events\n",
    "\n",
    "# Apply synthesis for all_sessions\n",
    "all_sessions_syn = []\n",
    "for evs, lbl, p in all_sessions:\n",
    "    evs2 = synthesize_timestamps_if_needed(evs, default_delta_ms=16.0)\n",
    "    all_sessions_syn.append((evs2, lbl, p))\n",
    "\n",
    "# Replace global all_sessions variable used by later cells\n",
    "all_sessions = all_sessions_syn\n",
    "\n",
    "print(\"Final session count available for downstream processing:\", len(all_sessions))\n",
    "# print a small sample\n",
    "for i, (evs, lbl, p) in enumerate(all_sessions[:5]):\n",
    "    print(f\"[SAMPLE {i}] label={lbl} path={p} n_events={len(evs)} first_event={evs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827adb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract sessions: 100%|██████████| 3143/3143 [23:16<00:00,  2.25it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-window shape: (1617919, 20) labels: [884789 733130]\n",
      "All 20 feature names:\n",
      " ['mean_speed', 'std_speed', 'max_speed', 'mean_acc', 'std_acc', 'max_acc', 'mean_abs_dx', 'std_dx', 'mean_abs_dy', 'std_dy', 'mean_angles', 'std_angles', 'pause_frac', 'bbox_aspect', 'path_len', 'speed_p25', 'speed_p50', 'speed_p75', 'median_dt', 'n_events']\n"
     ]
    }
   ],
   "source": [
    "# Cell A - Extract per-window feature vectors (20 features) and show names\n",
    "import numpy as np, math, os, joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "WINDOW = 10\n",
    "STRIDE = max(1, WINDOW // 2)\n",
    "\n",
    "# feature extractor (same as before)\n",
    "def extract_features_array(events):\n",
    "    xs=[]; ys=[]; ts=[]\n",
    "    for e in events:\n",
    "        xs.append(float(e[\"x\"])); ys.append(float(e[\"y\"])); ts.append(float(e[\"t\"]))\n",
    "    xs = np.array(xs); ys = np.array(ys); ts = np.array(ts)\n",
    "    if (np.diff(ts) < 0).any():\n",
    "        ts = np.cumsum(np.clip(np.diff(ts, prepend=ts[0]), 1.0, None))\n",
    "    if len(xs) < 3:\n",
    "        return np.zeros(20, dtype=float)\n",
    "    dx = np.diff(xs); dy = np.diff(ys); dt = np.diff(ts)\n",
    "    dt = np.where(dt == 0, 1.0, dt)\n",
    "    vx = dx/dt; vy = dy/dt\n",
    "    speed = np.sqrt(vx**2 + vy**2)\n",
    "    acc = np.diff(speed) if speed.size>1 else np.array([0.0])\n",
    "    angles=[]\n",
    "    for i in range(1, len(dx)):\n",
    "        x1,y1=dx[i-1],dy[i-1]; x2,y2=dx[i],dy[i]\n",
    "        a1=math.atan2(y1,x1); a2=math.atan2(y2,x2)\n",
    "        da=a2-a1\n",
    "        while da<=-math.pi: da+=2*math.pi\n",
    "        while da>math.pi: da-=2*math.pi\n",
    "        angles.append(da)\n",
    "    angles = np.array(angles) if len(angles)>0 else np.array([0.0])\n",
    "    pause_thresh = np.percentile(dt,75)*1.5\n",
    "    pause_frac = float((dt > pause_thresh).sum())/max(1,len(dt))\n",
    "    width = xs.max() - xs.min() if xs.size else 0.0\n",
    "    height= ys.max() - ys.min() if ys.size else 0.0\n",
    "    bbox_aspect = float(width/height) if height != 0 else 0.0\n",
    "    path_len = float(np.sum(np.sqrt(dx*dx + dy*dy)))\n",
    "    feats = [\n",
    "        float(np.mean(speed)), float(np.std(speed)), float(np.max(speed)),\n",
    "        float(np.mean(acc)), float(np.std(acc)), float(np.max(acc)),\n",
    "        float(np.mean(np.abs(dx))), float(np.std(dx)),\n",
    "        float(np.mean(np.abs(dy))), float(np.std(dy)),\n",
    "        float(np.mean(angles)), float(np.std(angles)),\n",
    "        float(pause_frac),\n",
    "        float(bbox_aspect),\n",
    "        float(path_len),\n",
    "        float(np.percentile(speed,25)),\n",
    "        float(np.percentile(speed,50)),\n",
    "        float(np.percentile(speed,75)),\n",
    "        float(np.median(dt)),\n",
    "        float(len(xs))\n",
    "    ]\n",
    "    return np.array(feats, dtype=float)\n",
    "\n",
    "feature_names = [\n",
    "    \"mean_speed\", \"std_speed\", \"max_speed\",\n",
    "    \"mean_acc\", \"std_acc\", \"max_acc\",\n",
    "    \"mean_abs_dx\", \"std_dx\",\n",
    "    \"mean_abs_dy\", \"std_dy\",\n",
    "    \"mean_angles\", \"std_angles\",\n",
    "    \"pause_frac\",\n",
    "    \"bbox_aspect\",\n",
    "    \"path_len\",\n",
    "    \"speed_p25\", \"speed_p50\", \"speed_p75\",\n",
    "    \"median_dt\",\n",
    "    \"n_events\"\n",
    "]\n",
    "\n",
    "# Build per-window arrays and keep session->windows mapping for later\n",
    "X_windows = []\n",
    "y_windows = []\n",
    "session_windows = []  # list of (list_of_window_features, label, path)\n",
    "\n",
    "for evs, label, path in tqdm(all_sessions, desc=\"Extract sessions\"):\n",
    "    feats_list = []\n",
    "    for i in range(0, max(1, len(evs) - WINDOW + 1), STRIDE):\n",
    "        w = evs[i:i+WINDOW]\n",
    "        f = extract_features_array(w)\n",
    "        feats_list.append(f)\n",
    "        X_windows.append(f)\n",
    "        y_windows.append(1 if label == \"bot\" else 0)\n",
    "    if len(feats_list) == 0:\n",
    "        f = extract_features_array(evs)\n",
    "        feats_list.append(f)\n",
    "        X_windows.append(f)\n",
    "        y_windows.append(1 if label == \"bot\" else 0)\n",
    "    session_windows.append((feats_list, 1 if label == \"bot\" else 0, path))\n",
    "\n",
    "X_windows = np.vstack(X_windows)\n",
    "y_windows = np.array(y_windows)\n",
    "print(\"Per-window shape:\", X_windows.shape, \"labels:\", np.bincount(y_windows))\n",
    "print(\"All 20 feature names:\\n\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c379353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low variance features removed: ['n_events']\n",
      "High-correlation feature pairs (>0.95): (showing up to 10) [('std_speed', 'mean_speed'), ('max_speed', 'mean_speed'), ('max_speed', 'std_speed'), ('std_acc', 'mean_speed'), ('std_acc', 'std_speed'), ('std_acc', 'max_speed'), ('max_acc', 'std_acc'), ('std_dx', 'mean_abs_dx'), ('std_dy', 'mean_abs_dy'), ('path_len', 'mean_abs_dy')]\n",
      "Top feature importances:\n",
      " median_dt      0.226451\n",
      "speed_p25      0.163722\n",
      "mean_speed     0.138580\n",
      "speed_p50      0.086216\n",
      "max_speed      0.083242\n",
      "std_speed      0.078602\n",
      "speed_p75      0.042191\n",
      "std_acc        0.032785\n",
      "mean_abs_dy    0.027059\n",
      "path_len       0.025292\n",
      "max_acc        0.024747\n",
      "std_dy         0.024086\n",
      "dtype: float64\n",
      "Low importance features removed: ['pause_frac', 'mean_angles', 'bbox_aspect', 'n_events']\n",
      "FINAL selected features (count=9): ['mean_speed', 'mean_acc', 'std_dx', 'mean_abs_dy', 'std_angles', 'speed_p25', 'speed_p50', 'speed_p75', 'median_dt']\n",
      "Saved selected feature list to mouse_selected_features.json\n"
     ]
    }
   ],
   "source": [
    "# Cell B - Feature selection\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dfX = pd.DataFrame(X_windows, columns=feature_names)\n",
    "dfy = pd.Series(y_windows, name=\"label\")\n",
    "\n",
    "# 1) low variance removal\n",
    "low_var_thresh = 1e-6\n",
    "low_var = dfX.var()[dfX.var() <= low_var_thresh].index.tolist()\n",
    "print(\"Low variance features removed:\", low_var)\n",
    "\n",
    "# 2) correlation-based candidates\n",
    "corr = dfX.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr_pairs = [(col, idx) for col in upper.columns for idx in upper.index if upper.loc[idx,col] > 0.95]\n",
    "print(\"High-correlation feature pairs (>0.95): (showing up to 10)\", high_corr_pairs[:10])\n",
    "\n",
    "# 3) quick RF importance\n",
    "Xtr_tmp, Xte_tmp, ytr_tmp, yte_tmp = train_test_split(dfX, dfy, test_size=0.2, stratify=dfy, random_state=42)\n",
    "sc = StandardScaler().fit(Xtr_tmp)\n",
    "Xtr_s = sc.transform(Xtr_tmp)\n",
    "rf_tmp = RandomForestClassifier(n_estimators=200, class_weight='balanced', n_jobs=6, random_state=42)\n",
    "rf_tmp.fit(Xtr_s, ytr_tmp)\n",
    "importances = pd.Series(rf_tmp.feature_importances_, index=dfX.columns).sort_values(ascending=False)\n",
    "print(\"Top feature importances:\\n\", importances.head(12))\n",
    "\n",
    "# remove near-zero importance features\n",
    "importance_thresh = 0.003  # be conservative\n",
    "low_imp = importances[importances <= importance_thresh].index.tolist()\n",
    "print(\"Low importance features removed:\", low_imp)\n",
    "\n",
    "# Build initial keep set\n",
    "keep = set(feature_names) - set(low_var) - set(low_imp)\n",
    "\n",
    "# For each high-correlation pair, drop the lower-importance one\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(i+1, len(feature_names)):\n",
    "        f1 = feature_names[i]; f2 = feature_names[j]\n",
    "        if upper.loc[f1, f2] > 0.95:\n",
    "            # keep the one with higher importance\n",
    "            if importances[f1] >= importances[f2]:\n",
    "                if f2 in keep: keep.remove(f2)\n",
    "            else:\n",
    "                if f1 in keep: keep.remove(f1)\n",
    "\n",
    "selected_features = [f for f in feature_names if f in keep]\n",
    "print(\"FINAL selected features (count={}):\".format(len(selected_features)), selected_features)\n",
    "\n",
    "# Save selected features\n",
    "import json\n",
    "json.dump({\"selected_features\": selected_features}, open(os.path.join(OUT_DIR, \"mouse_selected_features.json\"), \"w\"), indent=2)\n",
    "print(\"Saved selected feature list to mouse_selected_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild session_windows from raw dataset files (drop-in)\n",
    "import os, json, csv, sys, traceback\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(os.getcwd(), \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Where to search for datasets (add more paths here if your files live elsewhere)\n",
    "candidates = []\n",
    "# allow explicit env var override\n",
    "if os.environ.get(\"DATASET_ROOT\"):\n",
    "    candidates.append(os.environ[\"DATASET_ROOT\"])\n",
    "# common local locations\n",
    "candidates += [\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"Downloads\"),\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"ISIT_Dataset\"),\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"archive\"),\n",
    "    os.path.join(os.getcwd(), \"data\", \"raw\"),\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\", \"raw\"),\n",
    "    os.path.join(os.getcwd(), \"..\", \"datasets\"),\n",
    "    os.path.join(os.getcwd(), \"datasets\"),\n",
    "]\n",
    "# dedupe & keep existing paths only\n",
    "search_roots = [os.path.abspath(p) for p in dict.fromkeys(candidates) if p and os.path.exists(p)]\n",
    "if not search_roots:\n",
    "    # fallback to current working directory\n",
    "    search_roots = [os.getcwd()]\n",
    "\n",
    "print(\"Searching dataset roots:\", search_roots)\n",
    "\n",
    "# windowing parameters (events per window)\n",
    "EVENT_WINDOW = int(os.environ.get(\"EVENT_WINDOW\", 40))   # number of raw events per feature window\n",
    "EVENT_STRIDE  = int(os.environ.get(\"EVENT_STRIDE\", 20))  # stride in events\n",
    "MIN_EVENTS_PER_SESSION = 10   # if fewer than this, skip\n",
    "\n",
    "# helper: parse CSV (Balabit) expecting rows x,y[,t]\n",
    "def parse_csv_events(path):\n",
    "    ev = []\n",
    "    try:\n",
    "        with open(path, \"r\", newline='', encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            reader = csv.reader(fh)\n",
    "            # try to skip header if non-numeric\n",
    "            for row in reader:\n",
    "                if not row:\n",
    "                    continue\n",
    "                # keep only numeric columns\n",
    "                nums = []\n",
    "                for r in row:\n",
    "                    try:\n",
    "                        nums.append(float(r))\n",
    "                    except:\n",
    "                        pass\n",
    "                if len(nums) >= 2:\n",
    "                    x = float(nums[0]); y = float(nums[1])\n",
    "                    t = float(nums[2]) if len(nums) >= 3 else None\n",
    "                    d = {\"x\": x, \"y\": y}\n",
    "                    if t is not None: d[\"t\"] = t\n",
    "                    ev.append(d)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "    return ev\n",
    "\n",
    "# helper: parse JSON files from ISIT-like structure or generic arrays\n",
    "def parse_json_events(path):\n",
    "    ev = []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            j = json.load(fh)\n",
    "            # common patterns:\n",
    "            # 1) top-level has 'trace' list of dicts with position/time\n",
    "            if isinstance(j, dict):\n",
    "                # many ISIT files have 'trace' or 'events'\n",
    "                for key in (\"trace\", \"events\", \"eventsList\", \"data\"):\n",
    "                    if key in j and isinstance(j[key], list):\n",
    "                        for item in j[key]:\n",
    "                            # item may be {'position': {'x':..,'y':..}, 'timestamp':...}\n",
    "                            if isinstance(item, dict):\n",
    "                                # nested position\n",
    "                                pos = None\n",
    "                                if \"position\" in item and isinstance(item[\"position\"], dict):\n",
    "                                    pos = item[\"position\"]\n",
    "                                elif \"pos\" in item and isinstance(item[\"pos\"], dict):\n",
    "                                    pos = item[\"pos\"]\n",
    "                                elif \"x\" in item and \"y\" in item:\n",
    "                                    pos = item\n",
    "                                if pos:\n",
    "                                    x = pos.get(\"x\") or pos.get(\"X\") or pos.get(\"clientX\") or pos.get(\"pageX\")\n",
    "                                    y = pos.get(\"y\") or pos.get(\"Y\") or pos.get(\"clientY\") or pos.get(\"pageY\")\n",
    "                                    t = item.get(\"timestamp\") or item.get(\"time\") or item.get(\"ts\") or item.get(\"t\")\n",
    "                                    try:\n",
    "                                        if x is None or y is None:\n",
    "                                            continue\n",
    "                                        d = {\"x\": float(x), \"y\": float(y)}\n",
    "                                        if t is not None:\n",
    "                                            d[\"t\"] = float(t)\n",
    "                                        ev.append(d)\n",
    "                                    except Exception:\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    # sometimes item itself is [x,y,t] or [x,y]\n",
    "                                    if isinstance(item, list) and len(item) >= 2:\n",
    "                                        try:\n",
    "                                            x=float(item[0]); y=float(item[1]); t=float(item[2]) if len(item)>2 else None\n",
    "                                            d={\"x\":x,\"y\":y}\n",
    "                                            if t is not None: d[\"t\"]=t\n",
    "                                            ev.append(d)\n",
    "                                        except:\n",
    "                                            continue\n",
    "                        if ev:\n",
    "                            return ev\n",
    "                # pattern: file itself is {'user_id':..., 'trace': [{'event_name':..., 'position': {'x':..,'y':..}, 'timestamp':...}, ...]}\n",
    "                # if fallback: scan entire dict for lists of dicts with x,y\n",
    "                def scan_for_list(o):\n",
    "                    if isinstance(o, dict):\n",
    "                        for v in o.values():\n",
    "                            res = scan_for_list(v)\n",
    "                            if res:\n",
    "                                return res\n",
    "                    elif isinstance(o, list):\n",
    "                        # check if list contains dicts with x,y\n",
    "                        ok = True\n",
    "                        found = []\n",
    "                        for item in o:\n",
    "                            if isinstance(item, dict) and ((\"x\" in item and \"y\" in item) or (\"position\" in item)):\n",
    "                                # reuse logic above\n",
    "                                if \"position\" in item and isinstance(item[\"position\"], dict):\n",
    "                                    p = item[\"position\"]\n",
    "                                    x = p.get(\"x\"); y = p.get(\"y\"); t = item.get(\"timestamp\") or item.get(\"time\") or None\n",
    "                                    if x is not None and y is not None:\n",
    "                                        try:\n",
    "                                            d={\"x\":float(x),\"y\":float(y)}\n",
    "                                            if t is not None: d[\"t\"]=float(t)\n",
    "                                            found.append(d)\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                elif \"x\" in item and \"y\" in item:\n",
    "                                    try:\n",
    "                                        d={\"x\":float(item[\"x\"]),\"y\":float(item[\"y\"])}\n",
    "                                        if \"t\" in item: d[\"t\"]=float(item[\"t\"])\n",
    "                                        found.append(d)\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            elif isinstance(item, list) and len(item) >= 2:\n",
    "                                try:\n",
    "                                    x=float(item[0]); y=float(item[1]); t=float(item[2]) if len(item)>2 else None\n",
    "                                    d={\"x\":x,\"y\":y}\n",
    "                                    if t is not None: d[\"t\"]=t\n",
    "                                    found.append(d)\n",
    "                                except:\n",
    "                                    pass\n",
    "                            else:\n",
    "                                # nested lists\n",
    "                                res = scan_for_list(item)\n",
    "                                if res:\n",
    "                                    return res\n",
    "                        if found:\n",
    "                            return found\n",
    "                    return None\n",
    "                res = scan_for_list(j)\n",
    "                if res:\n",
    "                    return res\n",
    "            elif isinstance(j, list):\n",
    "                # list of events\n",
    "                for item in j:\n",
    "                    if isinstance(item, dict):\n",
    "                        if \"x\" in item and \"y\" in item:\n",
    "                            try:\n",
    "                                d={\"x\":float(item[\"x\"]),\"y\":float(item[\"y\"])}\n",
    "                                if \"t\" in item: d[\"t\"]=float(item[\"t\"])\n",
    "                                ev.append(d)\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif \"position\" in item and isinstance(item[\"position\"], dict):\n",
    "                            p=item[\"position\"]\n",
    "                            try:\n",
    "                                d={\"x\":float(p.get(\"x\")),\"y\":float(p.get(\"y\"))}\n",
    "                                if \"timestamp\" in item: d[\"t\"]=float(item.get(\"timestamp\"))\n",
    "                                ev.append(d)\n",
    "                            except:\n",
    "                                pass\n",
    "                    elif isinstance(item, list) and len(item)>=2:\n",
    "                        try:\n",
    "                            x=float(item[0]); y=float(item[1]); t=float(item[2]) if len(item)>2 else None\n",
    "                            d={\"x\":x,\"y\":y}\n",
    "                            if t is not None: d[\"t\"]=t\n",
    "                            ev.append(d)\n",
    "                        except:\n",
    "                            pass\n",
    "            # final fallback: nothing matched\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "    return ev\n",
    "\n",
    "# import feature extractor from your project (use relative import; ensure project root in sys.path)\n",
    "try:\n",
    "    # try importing as package\n",
    "    from backend.mouse_model import extract_features_from_events\n",
    "    print(\"Imported extract_features_from_events from backend.mouse_model\")\n",
    "except Exception:\n",
    "    try:\n",
    "        # try adding parent path and importing\n",
    "        sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "        from backend.mouse_model import extract_features_from_events\n",
    "        print(\"Imported extract_features_from_events after adjusting sys.path\")\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        raise RuntimeError(\"Could not import backend.mouse_model.extract_features_from_events. Ensure your repo is present and PYTHONPATH includes project root.\")\n",
    "\n",
    "# Walk dataset roots and collect candidate files\n",
    "found_files = []\n",
    "for root in search_roots:\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            if fn.lower().endswith(\".csv\") or fn.lower().endswith(\".json\"):\n",
    "                found_files.append(os.path.join(dirpath, fn))\n",
    "print(\"Found candidate files:\", len(found_files))\n",
    "\n",
    "# Heuristic label detection from filename/path\n",
    "def detect_label_from_path(p):\n",
    "    p_lower = p.lower()\n",
    "    bot_keywords = (\"bot\",\"gremlin\",\"fake\",\"robot\",\"attack\",\"malicious\",\"automation\",\"selenium\",\"botnet\")\n",
    "    for k in bot_keywords:\n",
    "        if k in p_lower:\n",
    "            return 1\n",
    "    # fallback: if path contains 'human' or 'real' treat as human\n",
    "    human_keywords = (\"human\",\"real\",\"benign\",\"genuine\")\n",
    "    for k in human_keywords:\n",
    "        if k in p_lower:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "session_windows = []   # list of (feats_list, label, path)\n",
    "skipped = 0\n",
    "processed = 0\n",
    "for fp in found_files:\n",
    "    try:\n",
    "        events = []\n",
    "        if fp.lower().endswith(\".csv\"):\n",
    "            events = parse_csv_events(fp)\n",
    "        else:\n",
    "            events = parse_json_events(fp)\n",
    "        if not events or len(events) < MIN_EVENTS_PER_SESSION:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # build overlapping event-chunks -> each chunk will be passed to feature extractor\n",
    "        n = len(events)\n",
    "        i = 0\n",
    "        feats_list = []\n",
    "        while i < n:\n",
    "            chunk = events[i:i+EVENT_WINDOW]\n",
    "            if len(chunk) >= max(3, min(EVENT_WINDOW, 3)):  # require at least some events\n",
    "                try:\n",
    "                    fv = extract_features_from_events(chunk)\n",
    "                    # if extractor returns None or invalid, skip\n",
    "                    if fv is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        feats_list.append(fv)\n",
    "                except Exception:\n",
    "                    # extractor might expect x,y,t - fallback: try to coerce dicts\n",
    "                    try:\n",
    "                        fv = extract_features_from_events([{\"x\":e.get(\"x\"), \"y\":e.get(\"y\"), \"t\":e.get(\"t\")} for e in chunk])\n",
    "                        if fv is not None:\n",
    "                            feats_list.append(fv)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            i += EVENT_STRIDE\n",
    "        if not feats_list:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        label = detect_label_from_path(fp)\n",
    "        session_windows.append((feats_list, label, fp))\n",
    "        processed += 1\n",
    "        if processed % 200 == 0:\n",
    "            print(\"Processed sessions:\", processed, \"skipped:\", skipped)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(\"Done. Sessions created:\", len(session_windows), \"skipped files:\", skipped)\n",
    "\n",
    "# Save session_windows to disk (so later cells can load after restart)\n",
    "# We'll save a compact representation: list of dicts with small arrays converted to lists (to keep np.save usable)\n",
    "simple_sessions = []\n",
    "for feats_list, label, path in session_windows:\n",
    "    # feats_list is list of lists/floats; convert to python lists\n",
    "    fl = [list(map(float, np.asarray(x).tolist())) for x in feats_list]\n",
    "    simple_sessions.append({\"features\": fl, \"label\": int(label), \"path\": path})\n",
    "\n",
    "# Save as npy (pickled) and JSON summary\n",
    "np_save_path = os.path.join(OUT_DIR, \"session_windows.npy\")\n",
    "json_save_path = os.path.join(OUT_DIR, \"session_windows.json\")\n",
    "np.save(np_save_path, simple_sessions, allow_pickle=True)\n",
    "with open(json_save_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump({\"n_sessions\": len(simple_sessions), \"sample\": simple_sessions[:5]}, fh, indent=2)\n",
    "\n",
    "print(\"Saved session_windows: npy ->\", np_save_path, \" json ->\", json_save_path)\n",
    "\n",
    "# Also restore in-memory variable for immediate use in notebook\n",
    "# convert back to same structure (feats_list, label, path)\n",
    "session_windows = [(sess[\"features\"], sess[\"label\"], sess[\"path\"]) for sess in simple_sessions]\n",
    "print(\"session_windows ready in memory with\", len(session_windows), \"sessions (each contains per-window feature lists).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C (patched) - Session-level train/test split, uses canonical feature names when needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os, json, traceback\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "# If you prefer the project-root data/processed, set OUT_DIR env var before running.\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(os.getcwd(), \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "SEQ_LEN = int(os.environ.get(\"SEQ_LEN\", 8))\n",
    "TEST_SIZE = float(os.environ.get(\"TEST_SIZE\", 0.20))\n",
    "RANDOM_STATE = int(os.environ.get(\"RANDOM_STATE\", 42))\n",
    "STRIDE = int(os.environ.get(\"SEQ_STRIDE\", 1))\n",
    "\n",
    "print(\"OUT_DIR:\", OUT_DIR, \"SEQ_LEN:\", SEQ_LEN, \"TEST_SIZE:\", TEST_SIZE)\n",
    "\n",
    "# ---------- canonical feature names (order must match extract_features_from_events) ----------\n",
    "canonical_feature_names = [\n",
    "    \"mean_speed\", \"std_speed\", \"max_speed\",\n",
    "    \"mean_acc\", \"std_acc\", \"max_acc\",\n",
    "    \"mean_abs_dx\", \"std_dx\",\n",
    "    \"mean_abs_dy\", \"std_dy\",\n",
    "    \"mean_angles\", \"std_angles\",\n",
    "    \"pause_frac\",\n",
    "    \"bbox_aspect\",\n",
    "    \"path_len\",\n",
    "    \"speed_p25\", \"speed_p50\", \"speed_p75\",\n",
    "    \"median_dt\",\n",
    "    \"n_events\"\n",
    "]\n",
    "\n",
    "# ---------- load selected features produced by Cell B ----------\n",
    "selected_file = os.path.join(OUT_DIR, \"mouse_selected_features.json\")\n",
    "selected_features_from_file = None\n",
    "if os.path.exists(selected_file):\n",
    "    try:\n",
    "        with open(selected_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            loaded = json.load(fh)\n",
    "            if isinstance(loaded, dict) and \"selected_features\" in loaded:\n",
    "                selected_features_from_file = loaded[\"selected_features\"]\n",
    "            elif isinstance(loaded, list):\n",
    "                selected_features_from_file = loaded\n",
    "            elif isinstance(loaded, dict) and \"selected\" in loaded and isinstance(loaded[\"selected\"], list):\n",
    "                selected_features_from_file = loaded[\"selected\"]\n",
    "        print(\"Loaded selected features from\", selected_file, \"-> count:\", 0 if selected_features_from_file is None else len(selected_features_from_file))\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Failed to parse\", selected_file, \"- will fallback to notebook variable or defaults.\")\n",
    "else:\n",
    "    print(\"No mouse_selected_features.json in OUT_DIR; will fallback to in-memory selected_features or all features.\")\n",
    "\n",
    "# ---------- ensure session_windows present ----------\n",
    "if \"session_windows\" not in globals() or session_windows is None:\n",
    "    raise RuntimeError(\"session_windows not found in memory. Run preceding cells that build session_windows before Cell C.\")\n",
    "\n",
    "# ---------- build or load feature_names ----------\n",
    "feature_names = globals().get(\"feature_names\", None)\n",
    "if feature_names is None:\n",
    "    seq_meta_path = os.path.join(OUT_DIR, \"sequence_meta.json\")\n",
    "    if os.path.exists(seq_meta_path):\n",
    "        try:\n",
    "            with open(seq_meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                seq_meta = json.load(fh)\n",
    "                feature_names = seq_meta.get(\"feature_names\", None)\n",
    "                if feature_names:\n",
    "                    print(\"Loaded feature_names from sequence_meta.json\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if feature_names is None:\n",
    "    # infer from first window\n",
    "    try:\n",
    "        sample_feats_list, _, _ = session_windows[0]\n",
    "        if len(sample_feats_list) == 0:\n",
    "            raise RuntimeError(\"First session has no windows; cannot infer feature_names\")\n",
    "        sample_vec = np.asarray(sample_feats_list[0])\n",
    "        feature_names = [f\"f{i}\" for i in range(sample_vec.shape[0])]\n",
    "        print(\"Inferred feature_names as default f0..fN-1 (N=%d)\" % len(feature_names))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Could not determine feature_names: \" + str(e))\n",
    "\n",
    "# ---------- if feature_names are generic f0.. and length matches canonical, replace with canonical names ----------\n",
    "is_generic = all(str(fn).startswith(\"f\") and str(fn)[1:].isdigit() for fn in feature_names)\n",
    "if is_generic and len(feature_names) == len(canonical_feature_names):\n",
    "    print(\"Detected generic feature_names (f0..). Substituting canonical feature names based on mouse_model.py\")\n",
    "    feature_names = canonical_feature_names.copy()\n",
    "else:\n",
    "    # if lengths mismatch but canonical length matches observed, consider substitution\n",
    "    if len(feature_names) != len(canonical_feature_names) and len(feature_names) == len(canonical_feature_names):\n",
    "        feature_names = canonical_feature_names.copy()\n",
    "\n",
    "print(\"Using feature_names (count={}): {}\".format(len(feature_names), feature_names if len(feature_names)<=20 else feature_names[:20]))\n",
    "\n",
    "# ---------- determine selected_features (file > notebook > all) ----------\n",
    "if selected_features_from_file:\n",
    "    selected_features = selected_features_from_file\n",
    "    # if indices provided, convert to names\n",
    "    if all(isinstance(x, (int, np.integer)) for x in selected_features):\n",
    "        try:\n",
    "            idxs_from_file = [int(x) for x in selected_features]\n",
    "            selected_features = [feature_names[i] for i in idxs_from_file]\n",
    "            print(\"mouse_selected_features.json contained indices; converted to names.\")\n",
    "        except Exception:\n",
    "            print(\"mouse_selected_features.json indices invalid; will fallback later.\")\n",
    "    else:\n",
    "        # validate names exist\n",
    "        missing = [f for f in selected_features if f not in feature_names]\n",
    "        if missing:\n",
    "            print(\"Warning: items in mouse_selected_features.json not found in feature_names:\", missing)\n",
    "            # If none matched but canonical_feature_names contains all selected, try canonical mapping\n",
    "            if all(f in canonical_feature_names for f in (selected_features_from_file or [])) and len(feature_names) == len(canonical_feature_names):\n",
    "                print(\"But selected features all exist in canonical list — using canonical mapping.\")\n",
    "                selected_features = [f for f in selected_features_from_file if f in canonical_feature_names]\n",
    "            else:\n",
    "                # remove missing entries and continue\n",
    "                selected_features = [f for f in selected_features if f in feature_names]\n",
    "else:\n",
    "    selected_features = globals().get(\"selected_features\", None) or feature_names\n",
    "    print(\"Using selected_features from notebook or defaulting to all features (count=%d)\" % len(selected_features))\n",
    "\n",
    "# ---------- map selected_features to indices ----------\n",
    "try:\n",
    "    idxs = [feature_names.index(f) for f in selected_features]\n",
    "except Exception:\n",
    "    # maybe selected_features are indices already\n",
    "    if all(isinstance(x, (int, np.integer)) for x in selected_features):\n",
    "        idxs = [int(x) for x in selected_features]\n",
    "        selected_features = [feature_names[i] for i in idxs]\n",
    "    else:\n",
    "        print(\"selected_features mismatch; falling back to all features\")\n",
    "        idxs = list(range(len(feature_names)))\n",
    "        selected_features = [feature_names[i] for i in idxs]\n",
    "\n",
    "print(\"Selected feature count:\", len(idxs), \"Selected features:\", selected_features)\n",
    "\n",
    "# ---------- create session ids and labels ----------\n",
    "session_ids = list(range(len(session_windows)))\n",
    "labels = [int(sw[1]) for sw in session_windows]\n",
    "\n",
    "# Stratified split at session level (fallback if stratify impossible)\n",
    "stratify_param = labels if len(set(labels)) > 1 else None\n",
    "try:\n",
    "    train_sids, test_sids = train_test_split(session_ids, test_size=TEST_SIZE, stratify=stratify_param, random_state=RANDOM_STATE)\n",
    "except Exception as e:\n",
    "    print(\"Stratified split failed:\", e, \"-> using non-stratified split\")\n",
    "    train_sids, test_sids = train_test_split(session_ids, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Train sessions:\", len(train_sids), \"Test sessions:\", len(test_sids))\n",
    "\n",
    "# ---------- build per-window arrays ----------\n",
    "def build_per_window_arrays(sids):\n",
    "    Xw = []\n",
    "    yw = []\n",
    "    for sid in sids:\n",
    "        feats_list, lab, _ = session_windows[sid]\n",
    "        for f in feats_list:\n",
    "            arr = np.asarray(f, dtype=float)\n",
    "            if arr.shape[0] < max(idxs)+1:\n",
    "                # skip windows that don't have expected dims\n",
    "                continue\n",
    "            Xw.append(arr[idxs])\n",
    "            yw.append(int(lab))\n",
    "    if len(Xw) == 0:\n",
    "        return np.zeros((0, len(idxs))), np.zeros((0,), dtype=int)\n",
    "    return np.vstack(Xw), np.array(yw, dtype=int)\n",
    "\n",
    "Xw_train, yw_train = build_per_window_arrays(train_sids)\n",
    "Xw_test, yw_test   = build_per_window_arrays(test_sids)\n",
    "print(\"Per-window shapes -> train:\", Xw_train.shape, \" test:\", Xw_test.shape)\n",
    "try:\n",
    "    print(\"Per-window label counts train:\", np.bincount(yw_train), \" test:\", np.bincount(yw_test))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- build sequence arrays (SEQ_LEN windows per sequence) ----------\n",
    "def build_sequences_for_sids(sids, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    Xs, ys, sid_map = [], [], []\n",
    "    for sid in sids:\n",
    "        feats_list, lab, _ = session_windows[sid]\n",
    "        if len(feats_list) == 0:\n",
    "            seq = np.zeros((seq_len, len(idxs)), dtype=float)\n",
    "            Xs.append(seq); ys.append(int(lab)); sid_map.append(sid)\n",
    "            continue\n",
    "        arr = np.vstack([np.asarray(f, dtype=float)[idxs] for f in feats_list])  # (n_windows, feat_dim)\n",
    "        n_windows = arr.shape[0]\n",
    "        if n_windows >= seq_len:\n",
    "            for i in range(0, n_windows - seq_len + 1, stride):\n",
    "                Xs.append(arr[i:i+seq_len]); ys.append(int(lab)); sid_map.append(sid)\n",
    "        else:\n",
    "            pad = np.zeros((max(0, seq_len - n_windows), arr.shape[1]), dtype=float)\n",
    "            seq = np.vstack([arr, pad])\n",
    "            Xs.append(seq); ys.append(int(lab)); sid_map.append(sid)\n",
    "    if len(Xs) == 0:\n",
    "        return np.zeros((0, seq_len, len(idxs))), np.zeros((0,), dtype=int), np.array([], dtype=int)\n",
    "    return np.asarray(Xs, dtype=float), np.asarray(ys, dtype=int), np.asarray(sid_map, dtype=int)\n",
    "\n",
    "Xseq_train, yseq_train, seq_sid_train = build_sequences_for_sids(train_sids, seq_len=SEQ_LEN, stride=STRIDE)\n",
    "Xseq_test, yseq_test, seq_sid_test   = build_sequences_for_sids(test_sids, seq_len=SEQ_LEN, stride=STRIDE)\n",
    "print(\"Seq shapes -> train:\", Xseq_train.shape, \" test:\", Xseq_test.shape)\n",
    "try:\n",
    "    print(\"Seq label counts train:\", np.bincount(yseq_train), \" test:\", np.bincount(yseq_test))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- save outputs to OUT_DIR (so downstream cells can load after kernel restart) ----------\n",
    "def safe_save(obj, name, is_json=False):\n",
    "    path = os.path.join(OUT_DIR, name if (name.endswith(\".npy\") or name.endswith(\".json\")) else (name + (\".json\" if is_json else \".npy\")))\n",
    "    try:\n",
    "        if is_json:\n",
    "            with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                json.dump(obj, fh, indent=2)\n",
    "        else:\n",
    "            np.save(path, obj, allow_pickle=True)\n",
    "        print(\"Saved\", path)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Failed to save\", path)\n",
    "\n",
    "# per-window arrays\n",
    "safe_save(Xw_train, \"Xw_train.npy\")\n",
    "safe_save(yw_train, \"yw_train.npy\")\n",
    "safe_save(Xw_test, \"Xw_test.npy\")\n",
    "safe_save(yw_test, \"yw_test.npy\")\n",
    "# sequences\n",
    "safe_save(Xseq_train, \"Xseq_train.npy\")\n",
    "safe_save(yseq_train, \"yseq_train.npy\")\n",
    "safe_save(Xseq_test, \"Xseq_test.npy\")\n",
    "safe_save(yseq_test, \"yseq_test.npy\")\n",
    "# split & meta\n",
    "split_info = {\"train_sids\": list(train_sids), \"test_sids\": list(test_sids), \"seq_len\": SEQ_LEN, \"selected_features\": selected_features, \"selected_indices\": idxs, \"feature_names\": feature_names}\n",
    "safe_save(split_info, \"session_split.json\", is_json=True)\n",
    "\n",
    "sequence_meta = {\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"feat_dim\": int(Xseq_train.shape[2]) if Xseq_train.size else len(idxs),\n",
    "    \"selected_features\": selected_features,\n",
    "    \"selected_indices\": idxs,\n",
    "    \"feature_names\": feature_names\n",
    "}\n",
    "safe_save(sequence_meta, \"sequence_meta.json\", is_json=True)\n",
    "\n",
    "print(\"Cell C complete — saved arrays and meta to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c117475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D (patched) - Train final RF and save scaler + model (robust, saves into OUT_DIR)\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import multiprocessing\n",
    "import json, traceback\n",
    "\n",
    "# ---------- CONFIG / OUT_DIR detection ----------\n",
    "def find_project_root(max_up=6):\n",
    "    from pathlib import Path\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up+1):\n",
    "        if (p / \"backend\").exists() or (p / \".git\").exists():\n",
    "            return str(p.resolve())\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return str(Path.cwd().resolve())\n",
    "\n",
    "ROOT = os.environ.get(\"PROJECT_ROOT\") or find_project_root()\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(ROOT, \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# ---------- sanity checks ----------\n",
    "required_vars = [\"Xw_train\", \"yw_train\", \"Xw_test\", \"yw_test\"]\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing variables in memory required for training: {missing}. Run Cell C first or load saved .npy files into notebook.\")\n",
    "\n",
    "# ensure numpy arrays\n",
    "Xw_train = np.asarray(globals()[\"Xw_train\"], dtype=float)\n",
    "Xw_test  = np.asarray(globals()[\"Xw_test\"], dtype=float)\n",
    "yw_train = np.asarray(globals()[\"yw_train\"], dtype=int)\n",
    "yw_test  = np.asarray(globals()[\"yw_test\"], dtype=int)\n",
    "\n",
    "print(\"Shapes: Xw_train\", Xw_train.shape, \"yw_train\", yw_train.shape, \"Xw_test\", Xw_test.shape, \"yw_test\", yw_test.shape)\n",
    "\n",
    "# ---------- handle edge cases ----------\n",
    "if Xw_train.size == 0 or Xw_test.size == 0:\n",
    "    raise RuntimeError(\"Empty training or test arrays (check session_windows / selected features).\")\n",
    "\n",
    "# choose n_jobs sensibly (leave 1 core free if possible)\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "n_jobs_env = os.environ.get(\"RF_N_JOBS\")\n",
    "try:\n",
    "    n_jobs = int(n_jobs_env) if n_jobs_env else max(1, cpu_count - 1)\n",
    "except Exception:\n",
    "    n_jobs = max(1, cpu_count - 1)\n",
    "\n",
    "print(\"Using n_jobs =\", n_jobs, \" (cpu_count reported as\", cpu_count, \")\")\n",
    "\n",
    "# ---------- scale features ----------\n",
    "scaler_rf = StandardScaler().fit(Xw_train)\n",
    "Xw_train_s = scaler_rf.transform(Xw_train)\n",
    "Xw_test_s  = scaler_rf.transform(Xw_test)\n",
    "\n",
    "# ---------- train RandomForest ----------\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=int(os.environ.get(\"RF_N_ESTIMATORS\", 300)),\n",
    "    class_weight=os.environ.get(\"RF_CLASS_WEIGHT\", \"balanced\"),\n",
    "    n_jobs=n_jobs,\n",
    "    random_state=int(os.environ.get(\"RANDOM_STATE\", 42))\n",
    ")\n",
    "\n",
    "print(\"Training final RF (selected features)...\")\n",
    "try:\n",
    "    rf_final.fit(Xw_train_s, yw_train)\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ---------- evaluation (per-window) ----------\n",
    "print(\"\\n--- RF per-window evaluation (test set) ---\")\n",
    "yhat = rf_final.predict(Xw_test_s)\n",
    "proba = None\n",
    "try:\n",
    "    proba = rf_final.predict_proba(Xw_test_s)[:, 1]\n",
    "except Exception:\n",
    "    # some sklearn classifiers can fail to provide predict_proba\n",
    "    pass\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(yw_test, yhat, digits=4))\n",
    "\n",
    "try:\n",
    "    cm = confusion_matrix(yw_test, yhat)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if proba is not None and len(np.unique(yw_test)) > 1:\n",
    "    try:\n",
    "        auc = roc_auc_score(yw_test, proba)\n",
    "        print(\"ROC AUC (per-window): {:.6f}\".format(auc))\n",
    "    except Exception:\n",
    "        print(\"ROC AUC computation failed.\")\n",
    "else:\n",
    "    print(\"ROC AUC skipped (no probabilities or single-class test set).\")\n",
    "\n",
    "# ---------- save artifacts ----------\n",
    "rf_path = os.path.join(OUT_DIR, \"mouse_rf.save\")\n",
    "scaler_path = os.path.join(OUT_DIR, \"mouse_scaler.save\")\n",
    "\n",
    "try:\n",
    "    joblib.dump(rf_final, rf_path)\n",
    "    joblib.dump(scaler_rf, scaler_path)\n",
    "    print(f\"Saved RF model -> {rf_path}\")\n",
    "    print(f\"Saved scaler -> {scaler_path}\")\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# also save a small metadata JSON (feature names / shapes) if available\n",
    "meta = {}\n",
    "if \"feature_names\" in globals():\n",
    "    meta[\"feature_names\"] = globals().get(\"feature_names\")\n",
    "if \"selected_features\" in globals():\n",
    "    meta[\"selected_features\"] = globals().get(\"selected_features\")\n",
    "meta.update({\n",
    "    \"Xw_train_shape\": list(Xw_train.shape),\n",
    "    \"Xw_test_shape\": list(Xw_test.shape),\n",
    "    \"yw_train_counts\": list(map(int, np.bincount(yw_train))) if yw_train.size else [],\n",
    "    \"yw_test_counts\": list(map(int, np.bincount(yw_test))) if yw_test.size else []\n",
    "})\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(OUT_DIR, \"mouse_rf_meta.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "    print(\"Saved mouse_rf_meta.json\")\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    print(\"Failed to save mouse_rf_meta.json\")\n",
    "\n",
    "print(\"Cell D complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992abe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell E (patched) - Train LSTM (sequence model), save mouse_lstm.h5, mouse_lstm.keras, mouse_lstm_scaler.save, mouse_lstm_meta.json\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- OUT_DIR / ROOT detection ----------\n",
    "def find_project_root(max_up=6):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up + 1):\n",
    "        if (p / \"backend\").exists() or (p / \".git\").exists():\n",
    "            return str(p.resolve())\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return str(Path.cwd().resolve())\n",
    "\n",
    "ROOT = os.environ.get(\"PROJECT_ROOT\") or find_project_root()\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(ROOT, \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# ---------- check for required saved arrays (Cell C outputs) ----------\n",
    "def load_npy(name):\n",
    "    p = os.path.join(OUT_DIR, name)\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "    return np.load(p, allow_pickle=True)\n",
    "\n",
    "try:\n",
    "    Xseq_train = load_npy(\"Xseq_train.npy\")\n",
    "    yseq_train = load_npy(\"yseq_train.npy\")\n",
    "    Xseq_test  = load_npy(\"Xseq_test.npy\")\n",
    "    yseq_test  = load_npy(\"yseq_test.npy\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load sequence arrays from OUT_DIR:\", e)\n",
    "    raise\n",
    "\n",
    "print(\"Loaded sequence arrays: Xseq_train\", Xseq_train.shape, \"yseq_train\", yseq_train.shape,\n",
    "      \"Xseq_test\", Xseq_test.shape, \"yseq_test\", yseq_test.shape)\n",
    "\n",
    "# ---------- TensorFlow import (fail gracefully) ----------\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks, optimizers, metrics\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"TensorFlow import failed. Install TensorFlow in the venv to train LSTM.\")\n",
    "\n",
    "# ---------- config / hyperparams (env override) ----------\n",
    "SEQ_LEN = int(os.environ.get(\"SEQ_LEN\", Xseq_train.shape[1]))\n",
    "FEAT_DIM = int(os.environ.get(\"FEAT_DIM\", Xseq_train.shape[2]))\n",
    "BATCH_SIZE = int(os.environ.get(\"LSTM_BATCH\", 64))\n",
    "EPOCHS = int(os.environ.get(\"LSTM_EPOCHS\", 40))\n",
    "PATIENCE = int(os.environ.get(\"LSTM_PATIENCE\", 6))\n",
    "LR = float(os.environ.get(\"LSTM_LR\", 1e-3))\n",
    "\n",
    "OUT_MODEL_H5 = os.path.join(OUT_DIR, \"mouse_lstm.h5\")\n",
    "OUT_MODEL_KERAS = os.path.join(OUT_DIR, \"mouse_lstm.keras\")\n",
    "OUT_SCALER = os.path.join(OUT_DIR, \"mouse_lstm_scaler.save\")\n",
    "OUT_META = os.path.join(OUT_DIR, \"mouse_lstm_meta.json\")\n",
    "\n",
    "print(\"SEQ_LEN:\", SEQ_LEN, \"FEAT_DIM:\", FEAT_DIM, \"BATCH_SIZE:\", BATCH_SIZE, \"EPOCHS:\", EPOCHS, \"LR:\", LR)\n",
    "\n",
    "# ---------- prepare flattened scaler (fit on training sequences flattened along time axis) ----------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# flatten sequences to (n_samples * seq_len, feat_dim) for per-feature scaling\n",
    "Xtr_flat = Xseq_train.reshape(-1, FEAT_DIM)\n",
    "Xte_flat = Xseq_test.reshape(-1, FEAT_DIM)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr_flat)\n",
    "\n",
    "def scale_seqs(X):\n",
    "    flat = X.reshape(-1, FEAT_DIM)\n",
    "    s = scaler.transform(flat)\n",
    "    return s.reshape(X.shape)\n",
    "\n",
    "Xseq_train_s = scale_seqs(Xseq_train).astype(np.float32)\n",
    "Xseq_test_s  = scale_seqs(Xseq_test).astype(np.float32)\n",
    "\n",
    "# save scaler now (joblib)\n",
    "joblib.dump(scaler, OUT_SCALER)\n",
    "print(\"Saved LSTM scaler ->\", OUT_SCALER)\n",
    "\n",
    "# ---------- build model (small, CPU-friendly) ----------\n",
    "def build_lstm_model(seq_len=SEQ_LEN, feat_dim=FEAT_DIM, dropout=0.2, lstm_units=64, dense_units=32):\n",
    "    inp = layers.Input(shape=(seq_len, feat_dim), name=\"seq_input\")\n",
    "    # Use a serializable Masking layer (no custom TF ops)\n",
    "    x = layers.Masking(mask_value=0.0, name=\"masking\")(inp)\n",
    "    x = layers.LSTM(lstm_units, return_sequences=False, name=\"lstm\")(x)\n",
    "    x = layers.BatchNormalization(name=\"batch_norm\")(x)\n",
    "    x = layers.Dropout(dropout, name=\"dropout1\")(x)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(dropout * 0.5, name=\"dropout2\")(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"mouse_lstm_model\")\n",
    "    return model\n",
    "\n",
    "model = build_lstm_model()\n",
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "auc_metric = metrics.AUC(name=\"auc\")\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[auc_metric])\n",
    "model.summary()\n",
    "\n",
    "# ---------- prepare training callbacks ----------\n",
    "es = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=PATIENCE, restore_best_weights=True, verbose=1)\n",
    "mc = callbacks.ModelCheckpoint(OUT_MODEL_H5, save_best_only=True, monitor=\"val_auc\", mode=\"max\", verbose=1)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=max(2, PATIENCE//2), min_lr=1e-6, verbose=1)\n",
    "\n",
    "# ---------- class weights (optional to handle imbalance) ----------\n",
    "try:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    classes = np.unique(yseq_train)\n",
    "    if len(classes) > 1:\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=yseq_train)\n",
    "        class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "        print(\"Class weight:\", class_weight)\n",
    "    else:\n",
    "        class_weight = None\n",
    "except Exception:\n",
    "    class_weight = None\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"Training LSTM (CPU-friendly)...\")\n",
    "history = None\n",
    "try:\n",
    "    history = model.fit(\n",
    "        Xseq_train_s, yseq_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[es, mc, rlr],\n",
    "        class_weight=class_weight,\n",
    "        verbose=2\n",
    "    )\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# ---------- save final model (if checkpoint didn't produce file) ----------\n",
    "if not os.path.exists(OUT_MODEL_H5):\n",
    "    try:\n",
    "        model.save(OUT_MODEL_H5)\n",
    "        print(\"Saved model to\", OUT_MODEL_H5)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Warning: failed to save model via model.save(h5)\")\n",
    "\n",
    "# Also save native Keras (.keras) which is preferred for portability\n",
    "try:\n",
    "    model.save(OUT_MODEL_KERAS)   # native Keras format (zip)\n",
    "    print(\"Saved native Keras model ->\", OUT_MODEL_KERAS)\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    print(\"Warning: failed to save native .keras model (this is non-fatal)\")\n",
    "\n",
    "# ---------- evaluate on test set ----------\n",
    "print(\"Evaluating on test set...\")\n",
    "res = model.evaluate(Xseq_test_s, yseq_test, batch_size=BATCH_SIZE, verbose=2)\n",
    "print(\"Test metrics (loss + auc):\", res)\n",
    "\n",
    "# ---------- save metadata ----------\n",
    "# build class counts safely\n",
    "train_vals, train_counts = np.unique(yseq_train, return_counts=True)\n",
    "test_vals, test_counts = np.unique(yseq_test, return_counts=True)\n",
    "meta = {\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"feat_dim\": FEAT_DIM,\n",
    "    \"model_file_h5\": os.path.abspath(OUT_MODEL_H5),\n",
    "    \"model_file_keras\": os.path.abspath(OUT_MODEL_KERAS) if os.path.exists(OUT_MODEL_KERAS) else None,\n",
    "    \"scaler_file\": os.path.abspath(OUT_SCALER),\n",
    "    \"train_shape\": list(Xseq_train_s.shape),\n",
    "    \"test_shape\": list(Xseq_test_s.shape),\n",
    "    \"class_counts_train\": {int(k): int(v) for k, v in zip(train_vals, train_counts)} if train_vals.size else {},\n",
    "    \"class_counts_test\":  {int(k): int(v) for k, v in zip(test_vals, test_counts)} if test_vals.size else {}\n",
    "}\n",
    "with open(OUT_META, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(meta, fh, indent=2)\n",
    "print(\"Saved LSTM metadata ->\", OUT_META)\n",
    "\n",
    "print(\"Cell E complete. LSTM artifacts written to OUT_DIR:\")\n",
    "print(\" -\", OUT_MODEL_H5)\n",
    "print(\" -\", OUT_MODEL_KERAS if os.path.exists(OUT_MODEL_KERAS) else \"(native .keras not saved)\")\n",
    "print(\" -\", OUT_SCALER)\n",
    "print(\" -\", OUT_META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b18fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell F (patched) - Ensemble RF + LSTM evaluation (robust: loads artifacts if missing, saves ensemble meta)\n",
    "import os, json, joblib, traceback\n",
    "import numpy as np\n",
    "\n",
    "# ---------- detect project root / OUT_DIR ----------\n",
    "from pathlib import Path\n",
    "def find_project_root(max_up=6):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up+1):\n",
    "        if (p / \"backend\").exists() or (p / \".git\").exists():\n",
    "            return str(p.resolve())\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return str(Path.cwd().resolve())\n",
    "\n",
    "ROOT = os.environ.get(\"PROJECT_ROOT\") or find_project_root()\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(ROOT, \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# ---------- helper to load numpy arrays saved by previous cells ----------\n",
    "def load_npy_if_missing(varname, filename):\n",
    "    if varname in globals():\n",
    "        return globals()[varname]\n",
    "    path = os.path.join(OUT_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required array {filename} not found at {path} and {varname} not in memory.\")\n",
    "    print(f\"Loading {filename} from disk\")\n",
    "    return np.load(path, allow_pickle=True)\n",
    "\n",
    "# ---------- load sequence & mapping arrays ----------\n",
    "Xseq_test = load_npy_if_missing(\"Xseq_test\", \"Xseq_test.npy\")\n",
    "yseq_test = load_npy_if_missing(\"yseq_test\", \"yseq_test.npy\")\n",
    "\n",
    "# seq_sid_test maps each sequence back to session id (used for session-level aggregation)\n",
    "if \"seq_sid_test\" in globals():\n",
    "    seq_sid_test = globals()[\"seq_sid_test\"]\n",
    "else:\n",
    "    ss_path = os.path.join(OUT_DIR, \"session_split.json\")\n",
    "    if os.path.exists(ss_path):\n",
    "        try:\n",
    "            ss = json.load(open(ss_path, \"r\", encoding=\"utf-8\"))\n",
    "            seq_sid_test = np.asarray(ss.get(\"seq_sid_test\") or ss.get(\"seq_sid_test.npy\") or [], dtype=int)\n",
    "        except Exception:\n",
    "            seq_sid_test = None\n",
    "    else:\n",
    "        seq_sid_test = None\n",
    "\n",
    "# ---------- load RF artifacts (scaler_rf + rf_final) ----------\n",
    "def load_joblib_if_missing(varname, filename):\n",
    "    if varname in globals() and globals()[varname] is not None:\n",
    "        return globals()[varname]\n",
    "    path = os.path.join(OUT_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required model file {filename} not found at {path} and {varname} not in memory.\")\n",
    "    print(f\"Loading {filename} from disk\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "try:\n",
    "    scaler_rf = load_joblib_if_missing(\"scaler_rf\", \"mouse_scaler.save\")\n",
    "    rf_final = load_joblib_if_missing(\"rf_final\", \"mouse_rf.save\")\n",
    "except Exception:\n",
    "    # try alternative names\n",
    "    try:\n",
    "        scaler_rf = load_joblib_if_missing(\"scaler_rf\", \"mouse_scaler.joblib\")\n",
    "    except Exception:\n",
    "        raise\n",
    "    rf_final = load_joblib_if_missing(\"rf_final\", \"mouse_rf.save\")\n",
    "\n",
    "# ---------- compute RF sequence-level probabilities ----------\n",
    "n_seq_test = int(Xseq_test.shape[0])\n",
    "seq_len = int(Xseq_test.shape[1])\n",
    "feat_dim = int(Xseq_test.shape[2])\n",
    "print(\"Xseq_test shape:\", Xseq_test.shape)\n",
    "\n",
    "# Flatten windows -> scale with scaler_rf -> predict proba -> reshape\n",
    "flat_windows = Xseq_test.reshape(-1, feat_dim)\n",
    "try:\n",
    "    flat_windows_scaled_for_rf = scaler_rf.transform(flat_windows)\n",
    "except Exception:\n",
    "    # if scaler expects different feature order/shape, attempt safe conversion (raise helpful error)\n",
    "    raise RuntimeError(f\"scaler_rf.transform failed for flat_windows of shape {flat_windows.shape}. Check that selected features used for RF match sequence features.\")\n",
    "\n",
    "rf_flat_probs = rf_final.predict_proba(flat_windows_scaled_for_rf)[:, 1]\n",
    "rf_probs_seq = rf_flat_probs.reshape(n_seq_test, seq_len)\n",
    "rf_seq_prob = rf_probs_seq.mean(axis=1)\n",
    "\n",
    "# ---------- prepare LSTM predictions ----------\n",
    "# load LSTM model & scaler if missing\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import load_model as tf_load_model\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise RuntimeError(\"TensorFlow import failed. Install TF to run LSTM inference.\")\n",
    "\n",
    "# load LSTM model if not in memory\n",
    "if \"model\" in globals() and getattr(globals()[\"model\"], \"predict\", None):\n",
    "    lstm_model = globals()[\"model\"]\n",
    "else:\n",
    "    # prefer model file names placed earlier\n",
    "    model_path_candidates = [\n",
    "        os.path.join(OUT_DIR, \"mouse_lstm.h5\"),\n",
    "        os.path.join(OUT_DIR, \"mouse_lstm.keras\")\n",
    "    ]\n",
    "    model_path = next((p for p in model_path_candidates if os.path.exists(p)), None)\n",
    "    if not model_path:\n",
    "        raise FileNotFoundError(\"LSTM model not found in memory nor mouse_lstm.h5/mouse_lstm.keras in OUT_DIR.\")\n",
    "    print(\"Loading LSTM model from:\", model_path)\n",
    "    lstm_model = tf_load_model(model_path)\n",
    "\n",
    "# load LSTM scaler used in Cell E (name: mouse_lstm_scaler.save)\n",
    "if \"scaler\" in globals():\n",
    "    lstm_scaler = globals()[\"scaler\"]\n",
    "else:\n",
    "    scaler_candidates = [\n",
    "        os.path.join(OUT_DIR, \"mouse_lstm_scaler.save\"),\n",
    "        os.path.join(OUT_DIR, \"mouse_lstm_scaler.joblib\"),\n",
    "        os.path.join(OUT_DIR, \"mouse_lstm_scaler.pkl\")\n",
    "    ]\n",
    "    scaler_path = next((p for p in scaler_candidates if os.path.exists(p)), None)\n",
    "    if not scaler_path:\n",
    "        raise FileNotFoundError(\"LSTM scaler not found in memory nor in OUT_DIR (mouse_lstm_scaler.save).\")\n",
    "    lstm_scaler = joblib.load(scaler_path)\n",
    "    print(\"Loaded LSTM scaler from:\", scaler_path)\n",
    "\n",
    "# scale sequences same as during training: flatten -> transform -> reshape\n",
    "flat_seq_test = Xseq_test.reshape(-1, feat_dim)\n",
    "flat_seq_test_s = lstm_scaler.transform(flat_seq_test)\n",
    "Xseq_test_s = flat_seq_test_s.reshape(Xseq_test.shape).astype(np.float32)\n",
    "\n",
    "# compute LSTM sequence-level probs\n",
    "try:\n",
    "    lstm_seq_prob = lstm_model.predict(Xseq_test_s, batch_size=int(os.environ.get(\"LSTM_BATCH\", 64)), verbose=1).ravel()\n",
    "except Exception:\n",
    "    # fallback: try smaller batch\n",
    "    lstm_seq_prob = lstm_model.predict(Xseq_test_s, batch_size=16, verbose=1).ravel()\n",
    "\n",
    "# ---------- ensemble ----------\n",
    "w_rf = float(os.environ.get(\"ENS_WEIGHT_RF\", 0.5))\n",
    "w_lstm = float(os.environ.get(\"ENS_WEIGHT_LSTM\", 0.5))\n",
    "ensemble_seq_prob = (w_rf * rf_seq_prob + w_lstm * lstm_seq_prob) / (w_rf + w_lstm)\n",
    "ensemble_seq_pred = (ensemble_seq_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "lstm_seq_pred = (lstm_seq_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "rf_seq_pred = (rf_seq_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "\n",
    "# ---------- evaluation (per-sequence) ----------\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\nLSTM seq report:\")\n",
    "print(classification_report(yseq_test, lstm_seq_pred, digits=4))\n",
    "try:\n",
    "    print(\"LSTM seq AUC:\", roc_auc_score(yseq_test, lstm_seq_prob))\n",
    "except Exception:\n",
    "    print(\"LSTM seq AUC: could not compute (single-class or error)\")\n",
    "\n",
    "print(\"\\nRF seq report (avg windows):\")\n",
    "print(classification_report(yseq_test, rf_seq_pred, digits=4))\n",
    "try:\n",
    "    print(\"RF seq AUC:\", roc_auc_score(yseq_test, rf_seq_prob))\n",
    "except Exception:\n",
    "    print(\"RF seq AUC: could not compute\")\n",
    "\n",
    "print(\"\\nENSEMBLE (avg) seq report:\")\n",
    "print(classification_report(yseq_test, ensemble_seq_pred, digits=4))\n",
    "try:\n",
    "    print(\"ENSEMBLE seq AUC:\", roc_auc_score(yseq_test, ensemble_seq_prob))\n",
    "except Exception:\n",
    "    print(\"ENSEMBLE seq AUC: could not compute\")\n",
    "\n",
    "# ---------- session-level evaluation (average sequence probs per session) ----------\n",
    "if seq_sid_test is None or len(seq_sid_test) != len(ensemble_seq_prob):\n",
    "    # attempt to load seq_sid_test from saved session_split.json\n",
    "    ss_path = os.path.join(OUT_DIR, \"session_split.json\")\n",
    "    if os.path.exists(ss_path):\n",
    "        try:\n",
    "            ss = json.load(open(ss_path, \"r\", encoding=\"utf-8\"))\n",
    "            seq_sid_test = np.asarray(ss.get(\"seq_sid_test\") or ss.get(\"seq_sid_test.npy\") or ss.get(\"seq_sid_test_list\") or [], dtype=int)\n",
    "            print(\"Loaded seq_sid_test from session_split.json\")\n",
    "        except Exception:\n",
    "            seq_sid_test = None\n",
    "\n",
    "if seq_sid_test is not None and len(seq_sid_test) == len(ensemble_seq_prob):\n",
    "    sess_ids = np.unique(seq_sid_test)\n",
    "    sess_true = []\n",
    "    sess_rf_prob = []\n",
    "    sess_lstm_prob = []\n",
    "    sess_ensemble_prob = []\n",
    "    for sid in sess_ids:\n",
    "        mask = (seq_sid_test == sid)\n",
    "        if np.sum(mask) == 0: \n",
    "            continue\n",
    "        true_label = int(yseq_test[mask][0])\n",
    "        sess_true.append(true_label)\n",
    "        sess_rf_prob.append(float(rf_seq_prob[mask].mean()))\n",
    "        sess_lstm_prob.append(float(lstm_seq_prob[mask].mean()))\n",
    "        sess_ensemble_prob.append(float(ensemble_seq_prob[mask].mean()))\n",
    "\n",
    "    sess_true = np.array(sess_true)\n",
    "    sess_rf_prob = np.array(sess_rf_prob)\n",
    "    sess_lstm_prob = np.array(sess_lstm_prob)\n",
    "    sess_ensemble_prob = np.array(sess_ensemble_prob)\n",
    "\n",
    "    print(\"\\nSession-level evaluation (averaging sequence probs per session):\")\n",
    "    print(\"\\nRF session-level:\")\n",
    "    rf_sess_pred = (sess_rf_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "    print(classification_report(sess_true, rf_sess_pred, digits=4))\n",
    "    try: print(\"RF session AUC:\", roc_auc_score(sess_true, sess_rf_prob))\n",
    "    except: print(\"RF session AUC: could not compute\")\n",
    "\n",
    "    print(\"\\nLSTM session-level:\")\n",
    "    lstm_sess_pred = (sess_lstm_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "    print(classification_report(sess_true, lstm_sess_pred, digits=4))\n",
    "    try: print(\"LSTM session AUC:\", roc_auc_score(sess_true, sess_lstm_prob))\n",
    "    except: print(\"LSTM session AUC: could not compute\")\n",
    "\n",
    "    print(\"\\nEnsemble session-level:\")\n",
    "    ens_sess_pred = (sess_ensemble_prob >= float(os.environ.get(\"ENS_THRESHOLD\", 0.5))).astype(int)\n",
    "    print(classification_report(sess_true, ens_sess_pred, digits=4))\n",
    "    try: print(\"Ensemble session AUC:\", roc_auc_score(sess_true, sess_ensemble_prob))\n",
    "    except: print(\"Ensemble session AUC: could not compute\")\n",
    "else:\n",
    "    print(\"\\nSkipping session-level evaluation: seq_sid_test unavailable or length mismatch.\")\n",
    "\n",
    "# ---------- save ensemble metadata ----------\n",
    "ens_meta = {\n",
    "    \"rf_weight\": w_rf,\n",
    "    \"lstm_weight\": w_lstm,\n",
    "    \"method\": os.environ.get(\"ENS_METHOD\", \"weighted_avg\"),\n",
    "    \"threshold\": float(os.environ.get(\"ENS_THRESHOLD\", 0.5)),\n",
    "    \"n_sequences_test\": int(len(ensemble_seq_prob)),\n",
    "    \"rf_seq_shape\": list(rf_seq_prob.shape),\n",
    "    \"lstm_seq_shape\": list(lstm_seq_prob.shape),\n",
    "    \"created_at\": __import__(\"datetime\").datetime.utcnow().isoformat() + \"Z\"\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"mouse_ensemble_meta.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(ens_meta, fh, indent=2)\n",
    "print(\"Saved ensemble meta to\", os.path.join(OUT_DIR, \"mouse_ensemble_meta.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell G - Visualization (inline display + save): ROC curves + Confusion Matrices for RF / LSTM / Ensemble\n",
    "import os, json, joblib, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure inline plotting when running in a Jupyter notebook (safe check)\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------- OUT_DIR discovery -----------------\n",
    "def find_project_root(max_up=6):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up+1):\n",
    "        if (p / \"backend\").exists() or (p / \".git\").exists():\n",
    "            return str(p.resolve())\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return str(Path.cwd().resolve())\n",
    "\n",
    "ROOT = os.environ.get(\"PROJECT_ROOT\") or find_project_root()\n",
    "OUT_DIR = os.environ.get(\"OUT_DIR\", os.path.join(ROOT, \"data\", \"processed\"))\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def load_npy(varname, fname):\n",
    "    if varname in globals():\n",
    "        return globals()[varname]\n",
    "    p = os.path.join(OUT_DIR, fname)\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"{fname} missing at {p}\")\n",
    "    return np.load(p, allow_pickle=True)\n",
    "\n",
    "def safe_joblib_load(varname, fname):\n",
    "    if varname in globals() and globals()[varname] is not None:\n",
    "        return globals()[varname]\n",
    "    path = os.path.join(OUT_DIR, fname)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return joblib.load(path)\n",
    "\n",
    "# ----------------- load data arrays -----------------\n",
    "Xseq_test = load_npy(\"Xseq_test\", \"Xseq_test.npy\")\n",
    "yseq_test = load_npy(\"yseq_test\", \"yseq_test.npy\")\n",
    "print(\"Loaded Xseq_test\", Xseq_test.shape, \"yseq_test\", yseq_test.shape)\n",
    "\n",
    "# optional seq->session mapping (for session-level plotting)\n",
    "seq_sid_test = globals().get(\"seq_sid_test\", None)\n",
    "ss_path = os.path.join(OUT_DIR, \"session_split.json\")\n",
    "if seq_sid_test is None and os.path.exists(ss_path):\n",
    "    try:\n",
    "        ss = json.load(open(ss_path, \"r\", encoding=\"utf-8\"))\n",
    "        seq_sid_test = np.asarray(ss.get(\"seq_sid_test\") or ss.get(\"seq_sid_test_list\") or [], dtype=int)\n",
    "        print(\"Loaded seq_sid_test from session_split.json\")\n",
    "    except Exception:\n",
    "        seq_sid_test = None\n",
    "\n",
    "# ----------------- load RF artifacts -----------------\n",
    "scaler_rf = globals().get(\"scaler_rf\", None) or safe_joblib_load(\"scaler_rf\", \"mouse_scaler.save\")\n",
    "rf_final = globals().get(\"rf_final\", None) or safe_joblib_load(\"rf_final\", \"mouse_rf.save\")\n",
    "if scaler_rf is None or rf_final is None:\n",
    "    raise RuntimeError(\"RF model or scaler missing. Ensure mouse_rf.save & mouse_scaler.save exist in OUT_DIR or are present in globals().\")\n",
    "\n",
    "# ----------------- load LSTM model & scaler -----------------\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import load_model as tf_load_model\n",
    "except Exception:\n",
    "    tf = None\n",
    "    tf_load_model = None\n",
    "\n",
    "lstm_model = globals().get(\"model\", None)\n",
    "if lstm_model is None:\n",
    "    m1 = os.path.join(OUT_DIR, \"mouse_lstm.h5\")\n",
    "    m2 = os.path.join(OUT_DIR, \"mouse_lstm.keras\")\n",
    "    model_path = m1 if os.path.exists(m1) else (m2 if os.path.exists(m2) else None)\n",
    "    if model_path and tf_load_model:\n",
    "        lstm_model = tf_load_model(model_path)\n",
    "        print(\"Loaded LSTM model from\", model_path)\n",
    "    else:\n",
    "        raise RuntimeError(\"LSTM model missing. Ensure mouse_lstm.h5 or mouse_lstm.keras exists in OUT_DIR.\")\n",
    "\n",
    "lstm_scaler = globals().get(\"lstm_scaler\", None) or safe_joblib_load(\"lstm_scaler\", \"mouse_lstm_scaler.save\")\n",
    "if lstm_scaler is None:\n",
    "    raise RuntimeError(\"LSTM scaler missing. Ensure mouse_lstm_scaler.save exists in OUT_DIR or is in globals().\")\n",
    "\n",
    "# ----------------- compute RF sequence probs -----------------\n",
    "n_seq = Xseq_test.shape[0]\n",
    "seq_len = Xseq_test.shape[1]\n",
    "fdim = Xseq_test.shape[2]\n",
    "\n",
    "flat_windows = Xseq_test.reshape(-1, fdim)\n",
    "flat_windows_scaled = scaler_rf.transform(flat_windows)\n",
    "rf_probs_flat = rf_final.predict_proba(flat_windows_scaled)[:, 1]\n",
    "rf_probs_seq = rf_probs_flat.reshape(n_seq, seq_len)\n",
    "rf_seq_prob = rf_probs_seq.mean(axis=1)\n",
    "\n",
    "# ----------------- compute LSTM sequence probs -----------------\n",
    "flat_seq = Xseq_test.reshape(-1, fdim)\n",
    "flat_seq_s = lstm_scaler.transform(flat_seq)\n",
    "Xseq_test_s = flat_seq_s.reshape(Xseq_test.shape).astype(\"float32\")\n",
    "\n",
    "# predict (show progress bar)\n",
    "lstm_seq_prob = lstm_model.predict(Xseq_test_s, batch_size=int(os.environ.get(\"LSTM_BATCH\", 64)), verbose=1).ravel()\n",
    "\n",
    "# ----------------- ensemble -----------------\n",
    "w_rf = float(os.environ.get(\"ENS_WEIGHT_RF\", 0.5))\n",
    "w_lstm = float(os.environ.get(\"ENS_WEIGHT_LSTM\", 0.5))\n",
    "ensemble_seq_prob = (w_rf * rf_seq_prob + w_lstm * lstm_seq_prob) / (w_rf + w_lstm)\n",
    "threshold = float(os.environ.get(\"ENS_THRESHOLD\", 0.5))\n",
    "\n",
    "rf_pred = (rf_seq_prob >= threshold).astype(int)\n",
    "lstm_pred = (lstm_seq_prob >= threshold).astype(int)\n",
    "ens_pred = (ensemble_seq_prob >= threshold).astype(int)\n",
    "\n",
    "# ----------------- plotting helpers -----------------\n",
    "def plot_and_save(fig, fname):\n",
    "    path = os.path.join(OUT_DIR, fname)\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    print(\"Saved figure ->\", path)\n",
    "    plt.show()\n",
    "\n",
    "# 1) ROC plots (one figure each)\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def single_roc_plot(y_true, y_score, title):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    a = auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{title} ROC (AUC={a:.4f})\")\n",
    "    return fig, a\n",
    "\n",
    "fig, a_rf = single_roc_plot(yseq_test, rf_seq_prob, \"RF (sequence-level)\")\n",
    "plot_and_save(fig, \"roc_rf_seq.png\")\n",
    "\n",
    "fig, a_lstm = single_roc_plot(yseq_test, lstm_seq_prob, \"LSTM (sequence-level)\")\n",
    "plot_and_save(fig, \"roc_lstm_seq.png\")\n",
    "\n",
    "fig, a_ens = single_roc_plot(yseq_test, ensemble_seq_prob, \"Ensemble (sequence-level)\")\n",
    "plot_and_save(fig, \"roc_ensemble_seq.png\")\n",
    "\n",
    "# 2) Confusion matrices (one figure each)\n",
    "def confmat_plot(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(ax=fig.gca(), cmap=None, colorbar=False)\n",
    "    plt.title(title)\n",
    "    return fig, cm\n",
    "\n",
    "fig, cm_rf = confmat_plot(yseq_test, rf_pred, \"RF confusion matrix (sequence)\")\n",
    "plot_and_save(fig, \"confmat_rf_seq.png\")\n",
    "\n",
    "fig, cm_l = confmat_plot(yseq_test, lstm_pred, \"LSTM confusion matrix (sequence)\")\n",
    "plot_and_save(fig, \"confmat_lstm_seq.png\")\n",
    "\n",
    "fig, cm_e = confmat_plot(yseq_test, ens_pred, \"Ensemble confusion matrix (sequence)\")\n",
    "plot_and_save(fig, \"confmat_ensemble_seq.png\")\n",
    "\n",
    "# 3) Print classification reports + AUCs\n",
    "print(\"\\n=== Metrics summary (sequence-level) ===\\n\")\n",
    "print(f\"RF AUC: {a_rf:.6f}\")\n",
    "print(classification_report(yseq_test, rf_pred, digits=4))\n",
    "print(f\"\\nLSTM AUC: {a_lstm:.6f}\")\n",
    "print(classification_report(yseq_test, lstm_pred, digits=4))\n",
    "print(f\"\\nEnsemble AUC: {a_ens:.6f}\")\n",
    "print(classification_report(yseq_test, ens_pred, digits=4))\n",
    "\n",
    "# 4) Session-level ROC (optional)\n",
    "if seq_sid_test is not None and len(seq_sid_test) == len(ensemble_seq_prob):\n",
    "    sess_ids = np.unique(seq_sid_test)\n",
    "    sess_true = []\n",
    "    sess_rf = []; sess_l = []; sess_e = []\n",
    "    for sid in sess_ids:\n",
    "        mask = (seq_sid_test == sid)\n",
    "        if mask.sum() == 0: \n",
    "            continue\n",
    "        sess_true.append(int(yseq_test[mask][0]))\n",
    "        sess_rf.append(float(rf_seq_prob[mask].mean()))\n",
    "        sess_l.append(float(lstm_seq_prob[mask].mean()))\n",
    "        sess_e.append(float(ensemble_seq_prob[mask].mean()))\n",
    "    sess_true = np.array(sess_true)\n",
    "    sess_e = np.array(sess_e)\n",
    "\n",
    "    fpr_s, tpr_s, _ = roc_curve(sess_true, sess_e)\n",
    "    a_sess = auc(fpr_s, tpr_s)\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr_s, tpr_s)\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Ensemble ROC (session-level) AUC={a_sess:.4f}\")\n",
    "    savepath = os.path.join(OUT_DIR, \"roc_ensemble_session.png\")\n",
    "    fig.savefig(savepath, bbox_inches=\"tight\")\n",
    "    print(\"Saved session-level ROC ->\", savepath)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nSkipping session-level ROC plot: seq_sid_test missing or length mismatch.\")\n",
    "\n",
    "print(\"\\nCell G complete. Figures displayed inline and saved to OUT_DIR.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
